{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'util'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mimport\u001b[39;00m SGD\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m init_seed\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdataset\u001b[39;00m \u001b[39mimport\u001b[39;00m CTDataset\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m CustomResNet18\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'util'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Testing script. Here, we load the trained model and export predicted\n",
    "    values, true values, and filepaths of images as .json.\n",
    "\n",
    "'''\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import yaml\n",
    "import glob\n",
    "from tqdm import trange\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "from util import init_seed\n",
    "from dataset import CTDataset\n",
    "from model import CustomResNet18\n",
    "import numpy as np\n",
    "from train import create_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to load model\n",
    "def load_model(cfg, model_path):\n",
    "    '''\n",
    "        Creates a model instance and loads the latest model state weights.\n",
    "    '''\n",
    "    model_instance = CustomResNet18(cfg['num_classes'])         # create an object instance of our CustomResNet18 class\n",
    "    overwrite = cfg['overwrite']\n",
    "\n",
    "    state = torch.load(open(model_path, 'rb'), map_location='cpu')\n",
    "    model_instance.load_state_dict(state['model'])\n",
    "\n",
    "    return model_instance\n",
    "\n",
    "\n",
    "# define test function (where the true labels, predicted labels, and filepaths are logged)\n",
    "def test(cfg, model, dataLoader):\n",
    "    '''\n",
    "        Function for selecting a subset of images to be logged into Comet. \n",
    "        Images taken are...?\n",
    "    '''\n",
    "    \n",
    "    device = cfg['device']\n",
    "    model.to(device)\n",
    "\n",
    "    # put model into eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # create empty lists for true labels,  predicted labels, and image filepaths\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    logits = []\n",
    "    scores = []\n",
    "    filepaths = []\n",
    "    \n",
    "    with torch.no_grad():               # don't calculate intermediate gradient steps: we don't need them, so this saves memory and is faster\n",
    "        for idx, (data, labels, image_paths) in enumerate(dataLoader):\n",
    "\n",
    "            # put data and labels on device\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            # add true labels to the true labels list\n",
    "            #import pdb; pdb.set_trace() # DEBUGGER\n",
    "            labels_np = labels.cpu().detach().numpy()\n",
    "            true_labels.extend(labels_np)\n",
    "\n",
    "            # forward pass\n",
    "            prediction = model(data)\n",
    "\n",
    "            logits.extend(prediction.cpu().detach().numpy().tolist())\n",
    "            # raise ValueError(logits)\n",
    "            scores.extend(prediction.softmax(dim=1).cpu().detach().numpy().tolist())\n",
    "            \n",
    "            # add predicted labels to the predicted labels list\n",
    "            pred_label = torch.argmax(prediction, dim=1)\n",
    "            pred_label_np = pred_label.cpu().detach().numpy()\n",
    "            pred_labels.extend(pred_label_np)\n",
    "\n",
    "            filepaths.extend(image_paths)\n",
    "\n",
    "            # print(len(pred_labels), len(true_labels), logits[-1], scores[-1], len(filepaths))\n",
    "\n",
    "    return pred_labels, true_labels, logits, scores, filepaths\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Argument parser for command-line arguments:\n",
    "    # python ct_classifier/train.py --config configs/exp_resnet18.yaml\n",
    "    parser = argparse.ArgumentParser(description='Test deep learning model.')\n",
    "    parser.add_argument('--config', help='Path to config file', default='configs/exp_resnet18.yaml')\n",
    "    parser.add_argument('--model_path', help='Path to model .pt file', default='')\n",
    "    parser.add_argument('--save_json_path', help='Path to .json output', default='')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # load config\n",
    "    print(f'Using config \"{args.config}\"')\n",
    "    cfg = yaml.safe_load(open(args.config, 'r'))\n",
    "    cfg['batch_size'] = 2048\n",
    "\n",
    "    # init random number generator seed (set at the start)\n",
    "    init_seed(cfg.get('seed', None))\n",
    "\n",
    "    # check if GPU is available\n",
    "    device = cfg['device']\n",
    "    if device != 'cpu' and not torch.cuda.is_available():\n",
    "        print(f'WARNING: device set to \"{device}\" but CUDA not available; falling back to CPU...')\n",
    "        cfg['device'] = 'cpu'\n",
    "\n",
    "    # initialize data loaders for training and validation set\n",
    "    dl_val = create_dataloader(cfg, split='val')\n",
    "\n",
    "    # initialize model\n",
    "    model = load_model(cfg, args.model_path)\n",
    "\n",
    "    # metrics\n",
    "    pred_labels, true_labels, logits, scores, filepaths = test(cfg, model, dl_val)\n",
    "\n",
    "    output_dict = {'pred_labels': [int(i) for i in pred_labels], \n",
    "                   'true_labels': [int(i) for i in true_labels],\n",
    "                   'logits':logits,\n",
    "                   'scores':scores,\n",
    "                   'filepaths':filepaths}\n",
    "\n",
    "    output_dict.keys()\n",
    "    output_dict['pred_labels'][0]\n",
    "    # import pdb; pdb.set_trace()\n",
    "    # save as a json\n",
    "    with open(args.save_json_path,'w') as f:\n",
    "        json.dump(output_dict, f)\n",
    "\n",
    "\n",
    "# to call the test.py script directly from cmd:\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv4e_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
